{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/davide_sartor/MANGO-lite\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%cd ..\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from mango.environments import frozen_lake\n",
    "from mango import Agent, Mango\n",
    "from frozen_lake_tests import utils_plot, utils_save, utils_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the environment\n",
    "map_base = 2\n",
    "map_scale = 4\n",
    "p_frozen = None\n",
    "one_shot = False\n",
    "\n",
    "plot_confront = True\n",
    "plot_training_evolution = False\n",
    "plot_qvalues = False\n",
    "ignore_agent = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_confront:\n\u001b[1;32m     19\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(dir_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparisons/\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m     utils_plot\u001b[38;5;241m.\u001b[39mplot_confront_loss_reward_avg(\n\u001b[1;32m     21\u001b[0m         agents\u001b[38;5;241m=\u001b[39m[normal_agents, mango_agents] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_agent \u001b[38;5;28;01melse\u001b[39;00m [mango_agents],\n\u001b[1;32m     22\u001b[0m         labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvanilla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmango\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_agent \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmango\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m         save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mcomparisons/compare_reward_avg.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mango_agent, normal_agent, run_id_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(mango_agents, normal_agents, run_ids):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot_confront:\n",
      "File \u001b[0;32m~/MANGO-lite/frozen_lake_tests/utils_plot.py:166\u001b[0m, in \u001b[0;36mplot_confront_loss_reward_avg\u001b[0;34m(agents, labels, colors, save_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_list, label, color \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(agents, labels, colors):\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]:\n\u001b[0;32m--> 166\u001b[0m         r_mean, r_ci, ep_mean, ep_ci \u001b[38;5;241m=\u001b[39m get_statistics(agent_list, \u001b[38;5;28meval\u001b[39m)\n\u001b[1;32m    167\u001b[0m         plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MANGO-lite/frozen_lake_tests/utils_plot.py:151\u001b[0m, in \u001b[0;36mplot_confront_loss_reward_avg.<locals>.get_statistics\u001b[0;34m(agents, eval)\u001b[0m\n\u001b[1;32m    147\u001b[0m     reward_logs\u001b[38;5;241m.\u001b[39mappend(smooth(agent\u001b[38;5;241m.\u001b[39mreward_log[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mreward_log[::\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m    148\u001b[0m     ep_len_logs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    149\u001b[0m         smooth(agent\u001b[38;5;241m.\u001b[39mepisode_length_log[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mepisode_length_log[::\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 151\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reward_logs)\n\u001b[1;32m    152\u001b[0m reward_logs \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mpad(r, (\u001b[38;5;241m0\u001b[39m, max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(r)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reward_logs]\n\u001b[1;32m    153\u001b[0m reward_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(reward_logs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load agent models one by one\n",
    "dir_path = utils_save.path_to_save_dir(map_base, map_scale, p_frozen, one_shot)\n",
    "files = sorted(os.listdir(dir_path + \"models/\"))\n",
    "mango_agent_files = [name for name in files if name.startswith(\"mango_agent\")]\n",
    "run_ids = [f\"run_{name[-9:-7]}\" for name in mango_agent_files]\n",
    "mango_agents = [\n",
    "    utils_save.load_from_file(dir_path + \"models/\" + file_name) for file_name in mango_agent_files\n",
    "]\n",
    "if ignore_agent:\n",
    "    normal_agents = mango_agents\n",
    "else:\n",
    "    normal_agent_files = [name for name in files if name.startswith(\"normal_agent\")]\n",
    "    normal_agents = [\n",
    "        utils_save.load_from_file(dir_path + \"models/\" + file_name)\n",
    "        for file_name in normal_agent_files\n",
    "    ]\n",
    "\n",
    "if plot_confront:\n",
    "    os.makedirs(dir_path + \"comparisons/\", exist_ok=True)\n",
    "    utils_plot.plot_confront_loss_reward_avg(\n",
    "        agents=[normal_agents, mango_agents] if not ignore_agent else [mango_agents],\n",
    "        labels=[\"vanilla\", \"mango\"] if not ignore_agent else [\"mango\"],\n",
    "        save_path=f\"{dir_path}comparisons/compare_reward_avg.pdf\",\n",
    "    )\n",
    "\n",
    "for mango_agent, normal_agent, run_id_str in zip(mango_agents, normal_agents, run_ids):\n",
    "    if plot_confront:\n",
    "        os.makedirs(dir_path + \"comparisons/\", exist_ok=True)\n",
    "        utils_plot.plot_confront_loss_reward(\n",
    "            agents=[normal_agent, mango_agent] if not ignore_agent else [mango_agent],\n",
    "            labels=[\"vanilla\", \"mango\"] if not ignore_agent else [\"mango\"],\n",
    "            save_path=f\"{dir_path}comparisons/compare_reward_{run_id_str}.pdf\",\n",
    "        )\n",
    "\n",
    "    if plot_training_evolution:\n",
    "        os.makedirs(dir_path + \"training/\", exist_ok=True)\n",
    "        if not ignore_agent:\n",
    "            utils_plot.plot_normal_agent_loss_reward(\n",
    "                normal_agent,\n",
    "                save_path=f\"{dir_path}training/train_results_{run_id_str}_normal_agent.pdf\",\n",
    "            )\n",
    "        utils_plot.plot_mango_agent_loss_reward(\n",
    "            mango_agent,\n",
    "            save_path=f\"{dir_path}training/train_results_{run_id_str}_mango_agent.pdf\",\n",
    "        )\n",
    "\n",
    "    if plot_qvalues:\n",
    "        os.makedirs(dir_path + \"qvalues/\", exist_ok=True)\n",
    "        if not ignore_agent:\n",
    "            normal_agent.reset()\n",
    "            trajectory, rewards = normal_agent.run_episode(\n",
    "                randomness=0.0, episode_length=4**map_scale\n",
    "            )\n",
    "            frozen_lake.plot_utils.plot_all_qvals_normal_agent(\n",
    "                normal_agent,\n",
    "                trajectory,\n",
    "                save_path=f\"{dir_path}qvalues/qvalues_{run_id_str}_normal_agent.pdf\",\n",
    "            )\n",
    "        mango_agent.reset()\n",
    "        trajectory, rewards = mango_agent.run_episode(randomness=0.0, episode_length=4**map_scale)\n",
    "        frozen_lake.plot_utils.plot_all_qvals_mango_agent(\n",
    "            mango_agent,\n",
    "            trajectory,\n",
    "            save_path=f\"{dir_path}qvalues/qvalues_{run_id_str}_mango_agent.pdf\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     window_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(window) \u001b[38;5;241m/\u001b[39m window\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconvolve(signal, window_array, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m annealing_ep, max_ep, _, _ \u001b[38;5;241m=\u001b[39m utils_sim\u001b[38;5;241m.\u001b[39mtrain_params(map_base, map_scale, p, one_shot)\n\u001b[1;32m     15\u001b[0m max_len \u001b[38;5;241m=\u001b[39m annealing_ep \u001b[38;5;241m+\u001b[39m max_ep\n\u001b[1;32m     16\u001b[0m p_frozen \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.3\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def smooth(signal, window=0.05):\n",
    "    window = max(3, int(len(signal) * window))\n",
    "    if len(signal) < 10:\n",
    "        return signal\n",
    "    signal = np.array([s for s in signal if s is not None])\n",
    "    window_array = np.ones(window) / window\n",
    "    return np.convolve(signal, window_array, mode=\"valid\")\n",
    "\n",
    "\n",
    "annealing_ep, max_ep, _, _ = utils_sim.train_params(map_base, map_scale, p, one_shot)\n",
    "max_len = annealing_ep + max_ep\n",
    "p_frozen = [1.0, 0.8, 0.5, 0.3]\n",
    "\n",
    "for p in p_frozen:\n",
    "    dir_path = utils_save.path_to_save_dir(map_base, map_scale, p, one_shot)\n",
    "    files = sorted(os.listdir(dir_path + \"models/\"))\n",
    "    mango_agent_files = [name for name in files if name.startswith(\"mango_agent\")]\n",
    "    normal_agent_files = [name for name in files if name.startswith(\"normal_agent\")]\n",
    "\n",
    "    def get_rew_statistics(agent_files):\n",
    "        reward_logs = []\n",
    "        for agent_file in agent_files:\n",
    "            agent: Agent | Mango = utils_save.load_from_file(dir_path + \"models/\" + agent_file)\n",
    "            rew = smooth(agent.reward_log[1::2])\n",
    "            reward_logs.append(np.pad(rew, (0, max_len - len(rew)), \"edge\"))\n",
    "        mean = np.mean(reward_logs, axis=0)\n",
    "        ci95 = 1.96 * np.std(reward_logs, axis=0) / np.sqrt(len(reward_logs))\n",
    "        return mean, (mean - ci95, mean + ci95)\n",
    "\n",
    "    mean, ci = get_rew_statistics(mango_agent_files)\n",
    "    plt.plot(mean, \"-\", label=f\"p={p} Mango\")\n",
    "    #plt.fill_between(range(len(mean)), ci[0], ci[1], alpha=0.3)\n",
    "    mean, ci = get_rew_statistics(normal_agent_files)\n",
    "    plt.plot(mean, \"--\", label=f\"p={p} Normal\", color=plt.gca().lines[-1].get_color())\n",
    "    #plt.fill_between(range(len(mean)), ci[0], ci[1], alpha=0.3)\n",
    "\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
