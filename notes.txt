lower gamma for options helps make the training more stable:
    quicker convergence of options to reasonable routines 
    more training examples at higher levels

exploration rate decay -> i.e. randomness(i-1) = randomness(i)**2 < randomness(i) 
    higher levels learn slower = should converge to deterministic policies more slowly 
    con: more layers = less effective exploration at lower layer 
    (i.e. 4x4 sometimes never gets the reward bc the low policy anneals too fast)

use deterministic beta and truncate global episode = learns deterministic transitions
    need less episodes to generalize
    quvalues change in blocks rather than gradually

workaround to somewhat address Vincent's suggestion about state "condensation"
